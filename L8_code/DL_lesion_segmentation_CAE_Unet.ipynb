{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retico/cmepda_medphys/blob/master/L8_code/Lecture8_demo_CAE_semantic_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXs3Fzs_RMs"
      },
      "source": [
        "# DL methods for semantic segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04HYmYOSQbS3"
      },
      "source": [
        "Semantic Segmentation is a classic Computer Vision problem which involves taking as input some raw data (e.g. 2D or 3D images) and converting them into a mask with regions of interest highlighted.\n",
        "\n",
        "**Objective**\n",
        "\n",
        "We will design and train a Convolutional Auto-Encoder (CAE) and then a U-net to segment mass lesions in mammograms.\n",
        "\n",
        "We will use the dataset made available at:\n",
        "\n",
        "https://drive.google.com/drive/folders/1gW2trBHf7Gw7zJKBYG32XP8mc2IbezdW?usp=drive_link\n",
        "\n",
        "You can either add this folder to your drive (\"Add shortcut to drive\") or download the large_sample_Im_segmented_ref.zip we wil use, which contains 177 mass examples (104 benign and 73 malignant masses) and their segmentation masks.\n",
        "\n",
        "**Legend of file names**\n",
        "\n",
        "Mass lesions represented in each image.pgm are malignant if the file name ends with “_1.pgm”, benign if it ends with “_2.pgm”, e.g.:\n",
        "\n",
        "0007p1_1_1.pgm contains a malignant mass\n",
        "\n",
        "0003f1_1_1_2.pgm contains a benign mass\n",
        "\n",
        "\n",
        "**Segmented masses.**\n",
        "\n",
        "The folders whose name ends with \"_ref\", e.g.\n",
        "\n",
        "small_sample_Im_segmented_ref/ and\n",
        "\n",
        "large_sample_Im_segmented_ref/\n",
        "\n",
        "contain for each original IMAGE or the small or large sample the IMAGE_resized.pgm and the IMAGE_mass_mask.pgm (which has the same size of the IMAGE_resized.pgm).\n",
        "These lesion segmentation masks have been generated by using a simplified version of the semi-automated approach for mass lesion segmentation presented in the paper by Delogu *et al.*, Comput Biol Med (2007) [[ref]](https://pubmed.ncbi.nlm.nih.gov/17383623/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9pGqDKO_do7"
      },
      "source": [
        "## Reading data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85dNXPXH5EKb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHMZbpSo5S-2"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/gdrive/MyDrive/CMEPDA_MedPhys_datasets/IMAGES/Mammography_masses/large_sample_Im_segmented_ref.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/large_sample_Im_segmented_ref/ | head -n 4"
      ],
      "metadata": {
        "id": "ci9XazNVNHwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp8a7yXiKvFQ"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/large_sample_Im_segmented_ref\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifm5HJj3KvFS"
      },
      "source": [
        "# Reading data from local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0HiBu1fKvFT"
      },
      "outputs": [],
      "source": [
        "#dataset_path = \" ... local path to ... /DATASETS/IMAGES/Mammography_masses/large_sample_Im_segmented_ref\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTXcVb4d_vT7"
      },
      "source": [
        "##  Dataset overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMKlrTJq6iCB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JikOsFAwACxu"
      },
      "source": [
        "We have two kinds of images: *_resized*, i.e. the images containing the mass lesions, and *_mass_mask*, i.e. the lesion masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMav8hGSKvFW"
      },
      "outputs": [],
      "source": [
        "os.path.join(dataset_path, \"0069p1_4_2_resized.pgm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNUFr8Rc6jj8"
      },
      "outputs": [],
      "source": [
        "im_0 = Image.open(os.path.join(dataset_path, \"0069p1_4_2_resized.pgm\"))\n",
        "im_1 = Image.open(os.path.join(dataset_path, \"0069p1_4_2_mass_mask.pgm\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im_0, cmap='bone')"
      ],
      "metadata": {
        "id": "34aJmCCDXs-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im_1, cmap='jet', alpha = 0.5)"
      ],
      "metadata": {
        "id": "ujZgnLcqYZwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(im_0, cmap='bone')\n",
        "plt.imshow(im_1, cmap='jet', alpha = 0.5)"
      ],
      "metadata": {
        "id": "nLd_qWjmXxig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-gK-WIeAbOC"
      },
      "source": [
        "## Reading the images in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By-YBLYq7I_9"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write a function to read alle images and masks as np.arrays and to return arrays with a shape suitable to train a DL model [i.e. A(n_img, size_x, size_y, 1)]. In additions, we rescale the image intensity in the [0,1]. To fasten the DL training phase, the images are resized to a smaller dimension, e.g. (64, 64)."
      ],
      "metadata": {
        "id": "XIbnQuT5AHkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpPSQbTP7R3a"
      },
      "outputs": [],
      "source": [
        "def read_dataset(dataset_path, x_id =\"_resized\", y_id=\"_mass_mask\", res_W = 64, res_H = 64):\n",
        "    fnames = glob.glob(os.path.join(dataset_path, f\"*{x_id}.pgm\"  ))\n",
        "    X = []\n",
        "    Y = []\n",
        "    for fname in fnames:\n",
        "        X.append(imread(fname)[:,:,np.newaxis])\n",
        "        Y.append(imread(fname.replace(x_id, y_id))[:,:,np.newaxis])\n",
        "    return resize(np.array(X), (np.array(X).shape[0],res_W,res_H,1), mode = 'reflect', order = 3 ), np.round(resize(np.array(Y), (np.array(X).shape[0],res_W,res_H,1), mode = 'reflect', order = 0)/255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGkZ2WaH7VuH"
      },
      "outputs": [],
      "source": [
        "X,Y = read_dataset(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21vmpLNyy1yC"
      },
      "outputs": [],
      "source": [
        "print(X.shape, Y.shape)\n",
        "print(X.min(), X.max(), Y.min(), Y.max())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[0].squeeze())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a91uUVx6iFuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(Y[0].squeeze())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_holQbN9iKlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCiheBdKV469"
      },
      "source": [
        "# Train and test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GI4VzmsOjVS"
      },
      "source": [
        "We split the dataset in a train and a test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z53TqLr8Rfxw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_ZhTzzFQ1OP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_LMQDVIRwg9"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlCOsvKN1gZM"
      },
      "source": [
        "We will use the *train set* (X_train) to train the model (allowing for an internal train-validation split). We will leave apart the *test set* (X_test) to evaluate the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0AStooEBK0N"
      },
      "source": [
        "# Defining and training a Convolutional Auto-Encoder (CAE)  model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_V-rwX6BPXm"
      },
      "source": [
        "We will design and train a convolutional autoencoder, inspired by the paper by (Liu et al,\n",
        "*Deep convolutional neural network and 3D deformable approach for tissue segmentation in musculoskeletal Magnetic Resonance Imaging*, Magn Reson Med 2018; 79(4):2379-2391, https://onlinelibrary.wiley.com/doi/10.1002/mrm.26841 where we can find an example of a possible architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xursHLl17qiB"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Conv2D, Conv2DTranspose, Input\n",
        "from keras.models import Model, load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(shape=(64,64,1)):\n",
        "    input_tensor = Input(shape=shape)\n",
        "    x = Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(input_tensor)\n",
        "    x = Conv2D(64, (3,3), strides=2,  padding='same', activation='relu')(x)\n",
        "\n",
        "    x = Conv2D(128, (3,3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    x = Conv2DTranspose(64, (3,3), strides=2, padding='same', activation='relu')(x)\n",
        "    x = Conv2DTranspose(32, (3,3), strides=2, padding='same',activation='relu')(x)\n",
        "    x = Conv2DTranspose(32, (3,3), strides=2, padding='same',activation='relu')(x)\n",
        "\n",
        "    out = Conv2D(1, (1,1), padding='same',activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(input_tensor, out)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "I6_NMkOw05LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VbopgOW8jHN"
      },
      "outputs": [],
      "source": [
        "model = make_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s473cqnwQodZ"
      },
      "source": [
        "We compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNyje0RLQcsU"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72L1r0_l6CzT"
      },
      "source": [
        "We can choose to save the model weights (save_best_only=True) that realized the best performance on the internal validation set (*early stop*) by means of a *callbacks*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"model_CAE.{epoch:02d}-{val_accuracy:.4f}.keras\", # Changed .h5 to .keras\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    save_freq='epoch')"
      ],
      "metadata": {
        "id": "oNFI0AYhYRBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJE-yzP6QF0p"
      },
      "source": [
        "To fit the model we need to make an additional split of the *train* set into a *train* and a *validation* sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ-ArPuX6-fn"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train,Y_train, validation_split=0.1, epochs=300, callbacks=[checkpoint])\n",
        "# with these settings only after 50 epochs the validation accuracy starts to improves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMEXhaAT3bEW"
      },
      "source": [
        "We can visualize the loss and the accuracy on train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqdssfPm7h5S"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['accuracy', 'val_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbjMWEWy5Qkx"
      },
      "source": [
        "We can load a saved model, and evaluate its performance on the test examples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "metadata": {
        "id": "xU31fYBmOkjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save a trained model in a specific folder:\n",
        "#!cp model_CAE.101-0.9822.keras  /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/model_CAE.101-0.9822.keras"
      ],
      "metadata": {
        "id": "9LvPE6JeZAlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/"
      ],
      "metadata": {
        "id": "sysoMzdEVCd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUwlHW3i-3wQ"
      },
      "outputs": [],
      "source": [
        "# We can load one of the models we have just saved (the best performing one) or one of the models we have trained in a previous session and stored somewhere, e.g.:\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/model_CAE.101-0.9822.keras .\n",
        "\n",
        "model = load_model(\"model_CAE.101-0.9822.keras\")\n",
        "\n",
        "# We can check its architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITX7enr-Agsr"
      },
      "source": [
        "We visualize the predicted segmentation on some images of the train set ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olnBs2QBBAcB"
      },
      "outputs": [],
      "source": [
        "# we select one random example from the train set\n",
        "idx = 11   # 14, 33, 98, 11\n",
        "xtrain = X_train[idx][np.newaxis,...]\n",
        "ytrain = Y_train[idx][np.newaxis,...]\n",
        "print(xtrain.shape)\n",
        "\n",
        "# and we plot the original image, the \"ground truth mask\" and the prediction of our model\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "ax1 = plt.subplot(1,3,1)\n",
        "plt.imshow(xtrain.squeeze())\n",
        "ax1.title.set_text('Original image')\n",
        "\n",
        "ax2 = plt.subplot(1,3,2)\n",
        "plt.imshow(ytrain.squeeze())\n",
        "ax2.title.set_text('True Lesion Mask')\n",
        "\n",
        "ax3 = plt.subplot(1,3,3)\n",
        "plt.imshow(model.predict(xtrain).squeeze()>0.2) # You can remove the threshold \">0.2\" and see the output\n",
        "ax3.title.set_text('Predicted Lesion Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXHwsl4VArdu"
      },
      "source": [
        "and on images of the test set (never seen by the CAE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQWKrcag8XAl"
      },
      "outputs": [],
      "source": [
        "idx= 30 #  1, 15     18, 13\n",
        "xtest = X_test[idx][np.newaxis,...]\n",
        "ytest = Y_test[idx][np.newaxis,...]\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(xtest.squeeze())\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(ytest.squeeze())\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(model.predict(xtest).squeeze()>0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbC-AWdRBSwr"
      },
      "source": [
        "# Evaluation of the performance\n",
        "\n",
        "We can quantify the segmentation performance on the train and test in terms of the Dice index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVZ98-ek_NL7"
      },
      "outputs": [],
      "source": [
        "def dice(pred, true, k = 1):\n",
        "    intersection = np.sum(pred[true==k]) * 2.0\n",
        "    dice = intersection / (np.sum(pred) + np.sum(true))\n",
        "    return dice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgt22cBnNkCF"
      },
      "source": [
        "We compute the Dice index for a single example, either of the train or test set:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx=67\n",
        "y_pred = model.predict(X_train[idx][np.newaxis,...]).squeeze()>0.2\n",
        "y_true = Y_train[idx].squeeze()\n",
        "\n",
        "dice(y_pred, y_true)"
      ],
      "metadata": {
        "id": "uYivPj1HdCOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHSjx_HnY6DJ"
      },
      "source": [
        "We can also compute the Dice index for all images in the np.arrays of the train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JmkVmsVAbcQ"
      },
      "outputs": [],
      "source": [
        "def dice_coef(pred, true, k = 1):\n",
        "    intersection = 2.0 *np.sum(pred * (true==k), axis=(1,2,3))\n",
        "    dice = intersection / (pred.sum(axis=(1,2,3)) + true.sum(axis=(1,2,3)))\n",
        "    return dice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape, X_train.shape"
      ],
      "metadata": {
        "id": "UQOLewAkyzVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dice_coef(Y_train,model.predict(X_train)>0.2)"
      ],
      "metadata": {
        "id": "qNoZdjoMWuQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7YM2lQ4A9v2"
      },
      "outputs": [],
      "source": [
        "dice_coef(Y_test,model.predict(X_test)>0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and compute their mean values:"
      ],
      "metadata": {
        "id": "rFVDrF3zeS5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dice_coef(Y_train,model.predict(X_train)>0.5).mean()"
      ],
      "metadata": {
        "id": "y0VMTlLFW653"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A3Q0KvWFWf4"
      },
      "outputs": [],
      "source": [
        "dice_coef(Y_test,model.predict(X_test)>0.5).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6iYdIIyZQuB"
      },
      "source": [
        "You can explore the dependence of the average Dice values on the threshold used to binarize the CAE output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wDwrtSoKvFl"
      },
      "source": [
        "# Defining and training a U-net  model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-net models are extremely powerful techniques to segment medical images. They were introduced by Ronnemberger *et al.* with the paper *U-net: Convolutional networks for biomedical image segmentation*. Lect Notes Comput Sci 9351:234–41 (2015)  [[Ref]](https://doi.org/10.1007/978-3-319-24574-4_28).\n",
        "\n",
        "There are a number of very informative tutorials on the web, e.g. [[Ref]](https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras/). The following U-net implementation is derived from the [tutorial](https://github.com/bnsreenu/python_for_image_processing_APEER) by S. Bhattiprolu on this topic."
      ],
      "metadata": {
        "id": "J_Lava-de6gE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfXHIa3_KvFl"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Activation, MaxPool2D, Concatenate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is practical to define functions for convolution, ancoding and decoding blocks:"
      ],
      "metadata": {
        "id": "k07t0wrqe-EJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOX3V-EXKvFl"
      },
      "outputs": [],
      "source": [
        "# Convolution block with 2 conv layers and batch normalization for each layer\n",
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)   #Not in the original network.\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)  #Not in the original network\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "# Encoder block: Conv block followed by maxpooling.\n",
        "# Returns both convolution and maxpooling outputs. The conv output can be used for concatenation (skip connections) with decoder.\n",
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "# Decoder Block: Skip features gets input from encoder for concatenation\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use these functions to build our model"
      ],
      "metadata": {
        "id": "D1GVb4pze_S3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwSuop0PKvFl"
      },
      "outputs": [],
      "source": [
        "def build_unet(input_shape, n_classes):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024) #Bridge\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    if n_classes == 1:\n",
        "      activation = 'sigmoid'\n",
        "    else:\n",
        "      activation = 'softmax'\n",
        "\n",
        "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This general formulation of the model allows to use it with input_shape=(256,256,3), similarly to the original implementation by  [[Ref]](https://github.com/bnsreenu/python_for_image_processing_APEER). It works also with an input_shape=(128,128,1) or (64,64,1)"
      ],
      "metadata": {
        "id": "l0ylPVjRfUtT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqymdSc4KvFm"
      },
      "outputs": [],
      "source": [
        "model_Unet = build_unet(input_shape=(64,64,1), n_classes=1)\n",
        "print(model_Unet.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This network has too many parameters, we can simplify the architecture a bit:"
      ],
      "metadata": {
        "id": "hC-H9ubO4bjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_shape, n_classes):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 8 )\n",
        "    s2, p2 = encoder_block(p1, 16)\n",
        "    s3, p3 = encoder_block(p2, 32)\n",
        "\n",
        "    b1 = conv_block(p3, 64) #Bridge\n",
        "\n",
        "    d1 = decoder_block(b1, s3, 32)\n",
        "    d2 = decoder_block(d1, s2, 16)\n",
        "    d3 = decoder_block(d2, s1, 8)\n",
        "\n",
        "    if n_classes == 1:\n",
        "      activation = 'sigmoid'\n",
        "    else:\n",
        "      activation = 'softmax'\n",
        "\n",
        "    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=activation)(d3)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "6lOZNYBc4UXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_Unet = build_unet(input_shape=(64, 64,1), n_classes=1)\n",
        "print(model_Unet.summary())"
      ],
      "metadata": {
        "id": "-YbDo9xU42pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ghPVaBVKvFo"
      },
      "outputs": [],
      "source": [
        "model_Unet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"model_Unet.{epoch:02d}-{val_accuracy:.4f}.keras\", # Changed .h5 to .keras\n",
        "    monitor='val_accuracy',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='auto', save_freq='epoch')"
      ],
      "metadata": {
        "id": "kmiZRKQ7a9c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_7NKY-_KvFo"
      },
      "outputs": [],
      "source": [
        "history = model_Unet.fit(X_train,Y_train, validation_split=0.1, epochs=300, callbacks=[checkpoint]) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dmzYOdBKvFo"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['accuracy', 'val_accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr"
      ],
      "metadata": {
        "id": "dJpIoRSTLjX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save a trained model in a specific folder:\n",
        "#!cp model_Unet.200-0.9878.keras /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/."
      ],
      "metadata": {
        "id": "y-avPOXccO6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/"
      ],
      "metadata": {
        "id": "edkqL2uwhMtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQpsaZOfKvFp"
      },
      "outputs": [],
      "source": [
        "# We can load one of the models we have just saved or one of the models we have trained in a previous session and stored somewhere, e.g.:\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/CMEPDA_MedPhys/models_DL_segm/model_Unet.200-0.9878.keras .\n",
        "\n",
        "model_Unet = load_model(\"model_Unet.200-0.9878.keras\")\n",
        "model_Unet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the predicted segmentation on some images of the train set ..."
      ],
      "metadata": {
        "id": "xAyhvYMrgGsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hzXCbSGKvFq"
      },
      "outputs": [],
      "source": [
        "# we select one random example from the train set\n",
        "idx = 11   # 11, 14, 33, 98\n",
        "xtrain = X_train[idx][np.newaxis,...]\n",
        "ytrain = Y_train[idx][np.newaxis,...]\n",
        "print(xtrain.shape)\n",
        "\n",
        "# and we plot the original image, the \"ground truth mask\" and the prediction of our model\n",
        "plt.figure(figsize=(14,4))\n",
        "\n",
        "ax1 = plt.subplot(1,3,1)\n",
        "plt.imshow(xtrain.squeeze())\n",
        "ax1.title.set_text('Original image')\n",
        "\n",
        "ax2 = plt.subplot(1,3,2)\n",
        "plt.imshow(ytrain.squeeze())\n",
        "ax2.title.set_text('True Lesion Mask')\n",
        "\n",
        "ax3 = plt.subplot(1,3,3)\n",
        "plt.imshow(model_Unet.predict(xtrain).squeeze()>0.2) # You can remove the threshold \">0.2\" and see the output\n",
        "ax3.title.set_text('Predicted Lesion Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and on images of the test set (never seen by the U-net)"
      ],
      "metadata": {
        "id": "FZvHVtNmgfcL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uREXl30KvFq"
      },
      "outputs": [],
      "source": [
        "idx= 1 #  1, 15, 30     18, 13\n",
        "xtest = X_test[idx][np.newaxis,...]\n",
        "ytest = Y_test[idx][np.newaxis,...]\n",
        "\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(xtest.squeeze())\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(ytest.squeeze())\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(model_Unet.predict(xtest).squeeze()>0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the performance"
      ],
      "metadata": {
        "id": "fPfItZw8amTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can quantify the segmentation performance on the train and test set in terms of the Dice index:"
      ],
      "metadata": {
        "id": "EIlJ51hmgxKW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXNnLIVaKvFq"
      },
      "outputs": [],
      "source": [
        "dice_coef(Y_train,model_Unet.predict(X_train)>0.5).mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dice_coef(Y_test,model_Unet.predict(X_test)>0.5)"
      ],
      "metadata": {
        "id": "UntOsqKOxIKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfO_Afp4KvFq"
      },
      "outputs": [],
      "source": [
        "dice_coef(Y_test,model_Unet.predict(X_test)>0.5).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A fair comparison between the performances of the two DL models we built for image segmentation can be done once both have been fully optimizes. The optimization of the DL model architecture and of the training parameters requires a long trial and error phase devote to identify the model with a suitable complexity to face our segmentation problem and accounting for the constraints provided by the limited size of the available dataset.\n"
      ],
      "metadata": {
        "id": "zAAaJhWRhBaz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaYmQZgv6JQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}