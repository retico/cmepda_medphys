{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/retico/cmepda_medphys/blob/master/L7_code/Lecture7_ML_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"FQoeOn2US4Mb"},"source":["# **Classification and cross-validation with sklearn**"]},{"cell_type":"markdown","metadata":{"id":"6Dl_N3ojAwPX"},"source":["We will implement ML methods to distinguish subjects with ASD from controls, based on brain features computed by means of the [FreeSurfer](https://surfer.nmr.mgh.harvard.edu/) segmentation software. A subsample of the large amount of features generated by Freesurfer for the [ABIDE I](http://fcon_1000.projects.nitrc.org/indi/abide/) data cohort is analyzed.  \n","\n","We will use  [pandas](https://pandas.pydata.org/) and at [sklearn](https://scikit-learn.org/stable/). Both the libraries are already installed on Colab. For some operation it will be necessary to convert the pandas DataFrame in a Numpy array."]},{"cell_type":"code","metadata":{"id":"C-EPcFMQUIuu"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.svm import SVC  # Support Vector Classification"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zGO70JoqS91d"},"source":["# Read the dataset\n"]},{"cell_type":"code","metadata":{"id":"4Mkm2b86UKTs"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","dataset_file = \"/content/gdrive/MyDrive/CMEPDA_MedPhys_datasets/FEATURES/Brain_MRI_FS_ABIDE/FS_features_ABIDE_males_someGlobals.csv\"\n","# check and modify the path of the FS_features_ABIDE_males_someGlobals.csv file you downloaded in your drive\n","df = pd.read_csv(dataset_file, sep=';')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lTVF194RCdvb"},"source":["As in previous example, we add a column with the *Site* information (we can derive it from the *FILE_ID*)"]},{"cell_type":"code","metadata":{"id":"mdVAIHTVZvI6"},"source":["df['Site'] = df['FILE_ID'].apply(lambda x: x.split('_')[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2k7p_pJUmgM"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9DFaFTcHH0xS"},"source":["As in previous examples, we make the DX_GROUP column more readable. This time we add a column with the readable labels and we keep the numerical labels [-1,1] which can directly be used in the classifier training process.\n"]},{"cell_type":"code","metadata":{"id":"72_rTHZGiRqZ"},"source":["df['DX_GROUP_STR'] = df.DX_GROUP.apply(lambda x: 'Controls' if x==-1 else 'ASD')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75IFndL3JhNv"},"source":["print(df.DX_GROUP_STR.unique())\n","print(df.DX_GROUP.unique())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xEo5oEvcFVeW"},"source":["We can count the entries grouped by the diagnostic group"]},{"cell_type":"code","metadata":{"id":"II4cI6H_O0On"},"source":["df.groupby('DX_GROUP_STR')['FILE_ID'].count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wXXVcCvYDUau"},"source":["We have a comparable number of subjects in the two diagnostic categories, which is fine for training a classifier."]},{"cell_type":"markdown","metadata":{"id":"_kqLVxUJuxH8"},"source":["# Binary classification: ASD vs. control subjects"]},{"cell_type":"code","metadata":{"id":"ZVk-ttXlPAy2"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qtEVn01QS9dd"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RDWnnBR7Gqfk"},"source":["We can select a set of features we suppose to be relevant for the diagnostic group prediction."]},{"cell_type":"code","metadata":{"id":"sLwCV22ATCZA"},"source":["features = ['AGE_AT_SCAN', 'lh_MeanThickness',\n","       'rh_MeanThickness', 'lhCortexVol', 'rhCortexVol',\n","       'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'TotalGrayVol',\n","       ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfYx205aS88g"},"source":["We split the data sample in the train and test subsets  "]},{"cell_type":"code","metadata":{"id":"kO2CDzmNPcZA"},"source":["train_set, test_set = train_test_split(df, test_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2C9tso_WTatm"},"source":["train_set[features]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g49OLXPxuwAD"},"source":["classifier = SVC(kernel='linear', probability=True)\n","classifier = classifier.fit(StandardScaler().fit_transform(train_set[features]), train_set['DX_GROUP'])\n","classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-esA1KYHUBL"},"source":["We can compute the classification accuracy"]},{"cell_type":"code","metadata":{"id":"YgqRdiChTrnW"},"source":["classifier.score(StandardScaler().fit_transform(test_set[features]), test_set['DX_GROUP'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvMA0dZPFF2h"},"source":["## k-fold cross Validation\n","\n","In training a ML model, data should be partitioned in train and test set. The k-fold Cross Validation (CV) scheme provides a robust estimate of the performance and its associated error. Usually k=5 or k=10 is implemented, depending on the dataset size and on the available computing resources."]},{"cell_type":"markdown","metadata":{"id":"xsJpwg8dE1ng"},"source":["We have to import the model, preprocessing and metric functions from the sklearn libraries.\n","For some operations it will be necessary to convert the pandas DataFrame in a Numpy array."]},{"cell_type":"code","metadata":{"id":"tNO6aapKVpLm"},"source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_curve, auc\n","import numpy as np\n","from numpy import interp\n","from sklearn.preprocessing import RobustScaler, StandardScaler\n","from sklearn.pipeline import Pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KS7TuDfbDoyX"},"source":["As we did before, we select the features we will use as predictors\n"]},{"cell_type":"code","metadata":{"id":"jPkR5mKtZIW6"},"source":["features = ['AGE_AT_SCAN', 'lh_MeanThickness',\n","       'rh_MeanThickness', 'lhCortexVol', 'rhCortexVol',\n","       'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'TotalGrayVol',\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBx4guRdgFRe"},"source":["As the features (i.e. volume and thickness measures) are in different ranges of values, we rescale them column-wise to have all them in the same range. We can apply a z-score transform, *i.e. with the sklearn.StandardScaler* or other normalization transforms, e.g the *sklearn.RobustScaler*, which removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile)."]},{"cell_type":"code","metadata":{"id":"tb4eyzB-WWgK"},"source":["X, y = df[features], df['DX_GROUP']\n","# X = StandardScaler().fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GqVxWDMQFq43"},"source":["We define a function which implements the k-fold CV, computes and averages the AUC values over the folds and provides plots of the ROC curve."]},{"cell_type":"code","metadata":{"id":"TFw70C77Vi1H"},"source":["def plot_cv_roc(X, y, classifier, n_splits=5, scaler=None):\n","  \"\"\"\n","  plot_cv_roc trains the classifier on X data with y labels, implements the\n","  k-fold-CV with k=n_splits, may implement a feature scaling function.\n","  It plots the ROC curves for each k fold and their average and displays\n","  the corresponding AUC values and the standard deviation over the k folders.\n","  \"\"\"\n","  if scaler:\n","    model = Pipeline([('scaler', scaler()),\n","                    ('classifier', classifier)])\n","  else:\n","    model = classifier\n","\n","  try:\n","    y = y.to_numpy()\n","    X = X.to_numpy()\n","  except AttributeError:\n","    pass\n","\n","  cv = StratifiedKFold(n_splits)\n","\n","  tprs = [] #True positive rate\n","  aucs = [] #Area under the ROC Curve\n","  interp_fpr = np.linspace(0, 1, 100)\n","  plt.figure()\n","  i = 0\n","  for train, test in cv.split(X, y):\n","    probas_ = model.fit(X[train], y[train]).predict_proba(X[test])\n","    # Compute ROC curve and area under the curve\n","    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n","#      print(f\"{fpr} - {tpr} - {thresholds}\\n\")\n","    interp_tpr = interp(interp_fpr, fpr, tpr)\n","    tprs.append(interp_tpr)\n","\n","    roc_auc = auc(fpr, tpr)\n","    aucs.append(roc_auc)\n","    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n","            label=f'ROC fold {i} (AUC = {roc_auc:.2f})')\n","    i += 1\n","  plt.legend()\n","  plt.xlabel('False Positive Rate (FPR)')\n","  plt.ylabel('True Positive Rate (TPR)')\n","  plt.show()\n","\n","  plt.figure()\n","  plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n","        label='Chance', alpha=.8)\n","\n","  mean_tpr = np.mean(tprs, axis=0)\n","  mean_tpr[-1] = 1.0\n","  mean_auc = auc(interp_fpr, mean_tpr)\n","  std_auc = np.std(aucs)\n","  plt.plot(interp_fpr, mean_tpr, color='b',\n","          label=f'Mean ROC (AUC = {mean_auc:.2f} $\\pm$ {std_auc:.2f})',\n","          lw=2, alpha=.8)\n","\n","  std_tpr = np.std(tprs, axis=0)\n","  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","  plt.fill_between(interp_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n","                  label=r'$\\pm$ 1 std. dev.')\n","\n","  plt.xlim([-0.01, 1.01])\n","  plt.ylim([-0.01, 1.01])\n","  plt.xlabel('False Positive Rate',fontsize=18)\n","  plt.ylabel('True Positive Rate',fontsize=18)\n","  plt.title('Cross-Validation ROC of SVM',fontsize=18)\n","  plt.legend(loc=\"lower right\", prop={'size': 15})\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cNKpoFmKFx7L"},"source":["*It's ok! We've just defined a function, no output is expected*"]},{"cell_type":"code","metadata":{"id":"_rwm8KmDg2rY"},"source":["help(plot_cv_roc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oj1qsZsnItLh"},"source":["classifier = SVC(kernel='linear', probability=True)\n","plot_cv_roc(X,y, classifier, 5, scaler=RobustScaler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uS_Tc7oBVKyl"},"source":["## Exploring datasample subsets\n","\n","If we are convinced that the heterogeneity introduced by the wide age range is excessive, we can reduce the number of subjects according to a predefined age range."]},{"cell_type":"code","metadata":{"id":"DHoV2Ay-VOsZ"},"source":["boxplot = df.boxplot(column='AGE_AT_SCAN', by='Site', showfliers=False)\n","boxplot.set_title('Box plot of subject\\'s age at scan')\n","boxplot.get_figure().suptitle('');\n","boxplot.set_ylabel('Age [y]')\n","\n","boxplot.set_xticklabels(labels=boxplot.get_xticklabels(), rotation=50);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNxmt4-BvofN"},"source":["### Age < threshold"]},{"cell_type":"code","metadata":{"id":"d73LlnomWYqi"},"source":["features = ['AGE_AT_SCAN', 'lh_MeanThickness',\n","       'rh_MeanThickness', 'lhCortexVol', 'rhCortexVol',\n","       'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'TotalGrayVol',\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljZQK-C4VDFZ"},"source":["reduced_df = df[df.AGE_AT_SCAN<20]\n","X, y = reduced_df[features], reduced_df['DX_GROUP']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FhU3PhYk9OL"},"source":["df.shape, X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zsqJIOw6WpUi"},"source":["classifier = SVC(kernel='linear', probability=True)\n","plot_cv_roc(X,y, classifier, 5, scaler=RobustScaler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9CV0yd-onad2"},"source":["### Similar sites\n","We can explore a subset of sites with similar age characteristics"]},{"cell_type":"code","metadata":{"id":"3SmzqB6FnaCv"},"source":["selected_sites = df[(df['Site'] == 'KKI') | (df['Site'] == 'Stanford') | (df['Site'] == 'UCLA')]\n","X, y = selected_sites[features], selected_sites['DX_GROUP']\n","df.shape, X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fhFEQNjwoVOb"},"source":["selected_sites.groupby(['DX_GROUP_STR','Site'])['FILE_ID'].count()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDgH53zynsvJ"},"source":["classifier = SVC(kernel='linear', probability=True)\n","plot_cv_roc(X,y, classifier, 5, scaler=RobustScaler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HnHrP70Mh7ue"},"source":["A classifier trained on data from site A which learnt to distinguish a subject's category according to a confounding variable, will not work on data from site B.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-PzQLhJJEHoE"},"source":["# Binary classification: Site A vs. site B\n"]},{"cell_type":"markdown","metadata":{"id":"yKFj-V4pash1"},"source":["Let's see whether the Site information is a confounding variable for ASD vs. Control classification. We evaluate the classification performance in site A vs. site B classification.\n","\n","The first thing we need to do is selecting the dataframe rows which are related to two different sites, e.g. KKI and Stanford."]},{"cell_type":"code","metadata":{"id":"8cdkAC7vv4tW"},"source":["two_sites = df[(df['Site'] == 'KKI') | (df['Site'] == 'Stanford')]\n","\n","two_sites.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z73fQfueJvo5"},"source":["features = ['AGE_AT_SCAN', 'lh_MeanThickness',\n","       'rh_MeanThickness', 'lhCortexVol', 'rhCortexVol',\n","       'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'TotalGrayVol',\n","]\n","\n","X = RobustScaler().fit_transform(two_sites[features])\n","y = two_sites['Site'].apply(lambda x: 1 if x=='KKI' else -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xM-EUFPQKHFS"},"source":["classifier = SVC(kernel='linear', probability=True, random_state=1)\n","plot_cv_roc(X, y, classifier, 5, scaler=RobustScaler)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziE3EuelHyBG"},"source":["# Accounting for confounders into the analysis"]},{"cell_type":"markdown","metadata":{"id":"jwQWPS5Cx_gz"},"source":["To mitigate the effect of the different acquisition sites on the features, we have to harmonize data across sites. We can attempt to normalize them by applying, for example, a per-site feature normalization (*sklearn.RobustScaler*)."]},{"cell_type":"code","metadata":{"id":"H1zAKdtmbXu2"},"source":["df_site1 = df[df.Site == 'KKI']\n","df_site2 = df[df.Site == 'Stanford']\n","df_site3 = df[df.Site == 'UCLA']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3uiwck8cVq9"},"source":["features = ['AGE_AT_SCAN', 'lh_MeanThickness',\n","       'rh_MeanThickness', 'lhCortexVol', 'rhCortexVol',\n","       'lhCerebralWhiteMatterVol', 'rhCerebralWhiteMatterVol', 'TotalGrayVol',\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZj5K7Tozqr1"},"source":["from sklearn.preprocessing import RobustScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXEMHk1Xb7rR"},"source":["X_site1 = RobustScaler().fit_transform(df_site1[features])\n","X_site2 = RobustScaler().fit_transform(df_site2[features])\n","X_site3 = RobustScaler().fit_transform(df_site3[features])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dy47rrylc73G"},"source":["X = np.concatenate((X_site1, X_site2, X_site3))\n","y = np.concatenate((df_site1['DX_GROUP'], df_site2['DX_GROUP'], df_site3['DX_GROUP']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmwRMOerM7Yj"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sn_6ooLMNYVN"},"source":["classifier = SVC(kernel='linear', probability=True)\n","plot_cv_roc(X, y, classifier, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHuYVRXyX5_z"},"source":["# Conclusions\n","If we obtain good/bad discrimination performance between two different diagnostic classes, are we sure the classifier is exploiting the right descriptive features?\n","\n","To know more about how to evaluate the effect of confounding variables in your data you can read the recent paper by Ferrari E, *et al.*, [*Dealing with confounders and outliers in classification medical studies: the Autism Spectrum Disorders case study*](https://www.sciencedirect.com/science/article/pii/S0933365719306086), Artif Intell Med 2020, 108, 101926. doi: 10.1016/j.artmed.2020.101926"]}]}